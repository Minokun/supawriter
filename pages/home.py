import streamlit as st

st.set_page_config(page_title="å¥‡èè°·", page_icon="ğŸš€", layout="wide")
st.sidebar.title("å…³äº")
st.sidebar.info("""
    Â©2024 è‹å·å¥‡èè°·æŠ€æœ¯æœ‰é™è´£ä»»å…¬å¸ ç‰ˆæƒæ‰€æœ‰ (https://www.apuqi.com/)
""")
st.sidebar.header("è”ç³»æˆ‘ä»¬")
with st.sidebar:
    st.markdown("""
        é‚®ç®±ï¼šapqchina@apuqi.com  
        åœ°å€: è‹å·å¸‚ç›¸åŸåŒºå…ƒå’Œè¡—é“ä¸‡é‡Œè·¯88å·3å·æ¥¼  
        ç”µè¯: 400-702-7002  
        ![è”ç³»äºŒç»´ç ](https://www.apuqi.com/uploads/96bfb1e66d7c1a04f8240d64b7a9b99.jpg)
    """)
st.subheader("Dr.Q å¥‡åšå£«ğŸŒŸ", divider='rainbow')
st.caption("""
           å·¥ä¸šAIä¸€ç«™å¼å¹³å°ï¼Œè‡´åŠ›äºæé«˜å·¥ä¸šåˆ¶é€ ä¸­çš„å„ä¸ªç¯èŠ‚æ•ˆç‡ï¼ŒåŒ…å«ç”Ÿæˆå¼LLMå¤§æ¨¡å‹å’Œç›¸å…³æ™ºèƒ½AIå·¥å…·ã€‚
           
           The Industrial AI One-Stop Platform is dedicated to enhancing the efficiency of various industrial manufacturing processes, encompassing generative large language models (LLM) and related intelligent AI tools."""
           )
st.info("""

        ğŸ†•ç®€ä»‹ï¼šå½“å‰é¡µé¢æ¥å…¥äº†å¥‡èè°·AIåŠ©æ‰‹ï¼Œå¯ä»¥æœç´¢äº’è”ç½‘å›ç­”ä½ çš„é—®é¢˜ï¼Œä¹Ÿå¯ä»¥å›ç­”å…¬å¸å…¬ç« ç®€ä»‹ï¼Œè¯·å’Œæˆ‘å¯¹è¯å§ï¼

        âš ï¸æ³¨æ„ï¼šå› ä¸ºæ¥å…¥äº†äº’è”ç½‘æœç´¢å’Œé˜¿æ™®å¥‡çš„çŸ¥è¯†åº“ï¼Œæœç´¢å’Œçˆ¬å–æ•°æ®éœ€è¦ä¸€äº›æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…ã€‚ç‚¹å‡»æ¸…ç©ºæŒ‰é’®é‡æ–°å¼€å§‹å¯¹è¯ã€‚
        """)
if st.button("æ¸…ç©º"):
    st.session_state.messages = []
# with st.container(height=600):

from settings import LLM_MODEL
import openai
client = openai.OpenAI(api_key=LLM_MODEL['fastgpt']['api_key'], base_url=LLM_MODEL['fastgpt']['base_url'])

if "messages" not in st.session_state:
    st.session_state.messages = []

# Display chat messages from history on app rerun
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Accept user input
if prompt := st.chat_input("What is up?"):
    # Display user message in chat message container
    with st.chat_message("user"):
        st.markdown(prompt)

    # Add user message to chat history
    st.session_state.messages.append({"role": "user", "content": prompt})
    
    with st.chat_message("assistant"):
        stream = client.chat.completions.create(
            model='qwen1.5-chat',
            messages=[
                {"role": m["role"], "content": m["content"]}
                for m in st.session_state.messages
            ],
            stream=True,
        )
        response = st.write_stream(stream)

    # Add user message to chat history
    st.session_state.messages.append({"role": "assistant", "content": response})